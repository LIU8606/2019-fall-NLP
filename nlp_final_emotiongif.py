# -*- coding: utf-8 -*-
"""NLP_Final_EmotionGIF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hq4EjcT3xI4IVvhkb3KkEziV2kpOAZsL

# LSTM for Text Classification

## Imports and Fetch Data
"""

# Import 需要的libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from keras.models import Model
from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Conv1D, MaxPooling1D
from keras.optimizers import RMSprop
from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
from keras.utils import to_categorical
from keras.callbacks import EarlyStopping
import nltk
from collections import Counter

train = pd.read_json("https://nlp-apple-2020.imfast.io/emotionGIF_eval/train_gold.json", lines=True)
# train[4:10]['categories']
df = pd.DataFrame(train) # 轉成pandas的dataframe
df.info() # 查看欄位等相關資訊
df1 = df[['text','reply', 'categories']] # 挑選所要分析的欄位
df1.head() # 查看前5筆資訊

"""## Definitions"""

CATEGORIES = ["agree", "applause", "awww", "dance", "deal_with_it", "do_not_want", "eww", "eye_roll", "facepalm", "fist_bump", "good_luck", "happy_dance", "hearts", "high_five", "hug", "idk", "kiss", "mic_drop", "no", "oh_snap", "ok", "omg", "oops", "please", "popcorn", "scared", "seriously", "shocked", "shrug", "sigh", "slow_clap", "smh", "sorry", "thank_you", "thumbs_down", "thumbs_up", "want", "win", "wink", "yawn", "yes", "yolo", "you_got_this"]
MAX_WORDS = 500
MAX_LEN = 300  # 自行設置，本次文本長度雖然較短，但我們先不調整大小，直接跑跑看

"""## Encode categories"""

def category_to_matrix(cates_list):
  # [input]: list of string list
  # [output]: bool list
  arr = np.zeros((len(cates_list), 43), dtype='float32')
  idx = 0
  for cates in cates_list:
    c_len = len(cates)
    for item in cates:
      # print(CATEGORIES.index(item))
      arr[idx, CATEGORIES.index(item)] = 1/c_len
    idx = idx + 1
  return arr

"""## Train/Test Data Split"""

X = df1[['text','reply']]
Y = df1.categories
Y = category_to_matrix(Y)
# le = LabelEncoder()
# Y = le.fit_transform(Y) # 這邊Y.shape = (資料數, )
# Y = Y.reshape(-1,1) # 將Y的shape轉換成： Y.shape= (資料數, 1)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15)
# X_train['text'].values.tolist()

"""## TweetTokenizer"""

def get_corpus(data):
  # [input]: dataframe [['text','reply']]
  # [output]: corpus: sentence list
  data_text = [sent.lower() for sent in data['text'].values.tolist()]
  data_reply = [sent.lower() for sent in data['reply'].values.tolist()]
  tw_tokenizer = nltk.TweetTokenizer()
  corpus_text = [tw_tokenizer.tokenize(sentence) for sentence in data_text]
  corpus_reply = [tw_tokenizer.tokenize(sentence) for sentence in data_reply]
  return (corpus_text, corpus_reply)

def get_vocab(corpus):
  # get vocabulary in all contents
  # [input]: corpus: sentence list
  # [output]: vocabulary
  corpus_text, corpus_reply = corpus
  vocabulary = Counter()
  for sentence in corpus_text:
    vocabulary.update(sentence)
  for sentence in corpus_reply:
    vocabulary.update(sentence)
  return vocabulary

def get_vocab_list(corpus, limit):
  # [input]: corpus: sentence list
  vocabulary = get_vocab(corpus).most_common(limit)
  return [token for token, freq in vocabulary]

def data_to_seq(corpus, vocab):
  # corpus = get_corpus(data)
  # vocab = get_vocab_list(corpus, 30)
  corpus_text, corpus_reply = corpus
  seqs = []
  # process corpus_text
  for sent in corpus_text:
    seq = []
    for word in sent:
      if word in vocab:
        seq.append(vocab.index(word))
    seqs.append(seq)
  # process corpus_reply
  idx = 0
  for sent in corpus_reply:
    # seqs[idx].append(MAX_WORDS-1) # seperate reply
    for word in sent:
      if word in vocab:
        seqs[idx].append(vocab.index(word))
    idx = idx + 1
  return seqs

"""## Build Corpus & Vocabulary"""

corpus = get_corpus(X_train)
vocab = get_vocab_list(corpus, MAX_WORDS-1)

"""## Preprocessing Data"""

seqs = data_to_seq(corpus, vocab)
# seqs[0:1]

sequences_matrix = sequence.pad_sequences(seqs,maxlen=MAX_LEN) # 確保所有序列具有相同的形狀

"""## Model"""

def RNN(lstm, den):
    inputs = Input(name='inputs',shape=[MAX_LEN])
    layer = Embedding(MAX_WORDS,50,input_length=MAX_LEN)(inputs)
    layer = LSTM(lstm)(layer)
    layer = Dense(den,name='FC1')(layer)
    layer = Activation('relu')(layer)
    layer = Dropout(0.5)(layer)
    layer = Dense(43,name='out_layer')(layer) # 最後一層參數值為類別數
    layer = Activation('softmax')(layer) #二元分類是'sigmoid'，多元分類上修改為'softmax'
    model = Model(inputs=inputs,outputs=layer)
    return model

histories = []
for lstm in [60]:
  for den in [16, 32, 64, 128]:
    model = RNN(lstm=lstm, den=den)
    model.summary()
    model.compile(
        loss = 'categorical_crossentropy', 
        optimizer=RMSprop(),metrics = ['accuracy'])

    # train model
    history = model.fit(sequences_matrix,Y_train,
              batch_size=128,
              epochs=100,
              validation_split=0.2,
              callbacks=[EarlyStopping( monitor='val_loss', min_delta=0.001, patience=5, mode='auto' )])
    histories.append( (lstm, den, history) )

import matplotlib.ticker as mtick
# print(history.history.keys())
def showChart(dtype):
  fisz = (5, 5)
  plt.figure(figsize=fisz, dpi=100)
  # summarize history for accuracy
  for h in histories:
    plt.plot(h[2].history[dtype]) 
  plt.title(dtype)
  plt.ylabel(dtype)
  plt.xlabel('epoch')
  plt.legend([ 'lstm=' + str(h[0]) + ', dense=' + str(h[1]) for h in histories], loc='upper left')
  ax = plt.subplot(111)
  ax.xaxis.set_major_formatter(mtick.FormatStrFormatter('%2.0f'))
  plt.show()

# summarize history for loss plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('model loss')
showChart('val_accuracy')
showChart('val_loss')

"""## Evaluate Results"""

corpus = get_corpus(X_test)
test_sequences = data_to_seq(corpus, vocab)
test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=MAX_LEN)
# 使用測試集來進行模型評估
accr = model.evaluate(test_sequences_matrix,Y_test)

print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

preds = model.predict(np.array(test_sequences_matrix))
preds[:10]

"""## DEV TEST"""

def get_cate(preds):
  categories = []
  for pred in preds:
    pred_top_six = pred.argsort()[-6:][::-1]
    categories.append([ CATEGORIES[i] for i in pred_top_six ])
  return categories

import zipfile
def dev_test(data):
  # test[4:10]['categories']
  df = pd.DataFrame(data) # 轉成pandas的dataframe
  df.info() # 查看欄位等相關資訊
  df1 = df[['idx', 'text', 'reply']] # 挑選所要分析的欄位
  df1.head() # 查看前5筆資訊
  
  D_test = df1[['text', 'reply']]
  # 處理測試集資料
  corpus = get_corpus(D_test)
  test_sequences = data_to_seq(corpus, vocab)
  test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=MAX_LEN)
  # 使用測試集來進行模型評估
  preds = model.predict(np.array(test_sequences_matrix))
  # OUTPUT JSON 
  # {"idx": 32, "categories": ["eye_roll", "facepalm", "fist_bump", "good_luck", "popcorn", "happy_dance"], "reply": "Ouch!", "text": "Fell right under my trap"}
  categories = get_cate(preds)
  df['categories'] = categories
  # output json file
  df.to_json(r'eval.json', orient='records', lines=True)
  # zip file
  with zipfile.ZipFile('eval.zip', 'w') as zf:
    zf.write('eval.json')
  return

test_data = pd.read_json("https://nlp-apple-2020.imfast.io/emotionGIF_eval/test_unlabeled.json", lines=True)
dev_test(test_data)