# -*- coding: utf-8 -*-
"""lab1-0856034.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/198Scjxbv8lapwYdY5SX_XUVqYW8sDun7
"""



"""#Recommend Similar News Articles
This notebook demonstrates how to use bag-of-word vectors and cosine similarity for news article recommendation.
"""

import re
import math
import pandas as pd
import numpy as np
from collections import Counter

"""#Fetching the Corpus
`get_corpus()` reads the CSV file, and then return a list of the news headlines
"""

def get_corpus():
  df = pd.read_csv('https://raw.githubusercontent.com/bshmueli/108-nlp/master/reuters.csv') # https://bit.ly/nlp-reuters
  print("Dataset columns", df.columns)
  print("Dataset size", len(df))
  corpus = [df.content[i].lower() for i in range(len(df))]
  title_corpus = df.title.to_list()
  return corpus,title_corpus

def tokenize(document):
  words = re.split('\W+', document)
  #words = document.split(' ')
  #words = re.sub(r'[^\w\s]','',document)
  return words

"""# Read stop words
I add ' ' to the stoplist given by TA
"""

import urllib
stop_list=[]
url = "https://raw.githubusercontent.com/bshmueli/108-nlp/master/stopwords.txt"
file = urllib.request.urlopen(url)
for line in file:
  stop_list.append((line.decode("utf-8").strip('\n')))
stop_list.append('')
print(stop_list)

"""#Computing word frequencies
`get_vocab(corpus)` computes the word frequencies in a given corpus. It returns a list of 2-tuples. Each tuple contains the token and its frequency.

`remove_stop(vocab,stop_list)` remove the stop words in the `vocab`.
"""

def get_vocab(corpus):
  vocabulary = Counter()
  for document in corpus:
    tokens = tokenize(document)
    vocabulary.update(tokens)
  return vocabulary

def remove_stop(vocab,stop_list):
  temp = vocab.copy()
  #print(temp)
  for i in range(len(temp)):
    if temp[i][0] in stop_list:
      #print(temp[i][0])
      vocab.remove((temp[i][0],temp[i][1]))

"""#Compute TF_IDF Vector
$w = tf * log(N/df)$
"""

def df_list():
  df = []
  for token,freq in vocab:
    df_count = 0
    for i in range(len(corpus)):
      if token in corpus[i]:
        df_count +=1
    df.append(df_count)

  return df

def TFIDFvec(doc):
  words = tokenize(doc)

  tf = [words.count(token) for token,freq in vocab ]
  tf = np.array(tf)
  tf = tf/len(words)
  tfidf = tf * np.log(len(corpus)/df)

  return list(tfidf)

"""Cosine similarity between two numerical vectors"""

def cosine_similarity(vec_a, vec_b):
  assert len(vec_a) == len(vec_b)
  if sum(vec_a) == 0 or sum(vec_b) == 0:
    return 0 # hack
  a_b = sum(i[0] * i[1] for i in zip(vec_a, vec_b))
  a_2 = sum([i*i for i in vec_a])
  b_2 = sum([i*i for i in vec_b])
  return a_b/(math.sqrt(a_2) * math.sqrt(b_2))

def doc_similarity(doc_a, doc_b):
  return cosine_similarity(TFIDFvec(doc_a), TFIDFvec(doc_b))

"""# Find Similar Documents
Find and print the $k$ most similar titles to a given title
"""

def k_similar(seed_id, k):
  seed_doc = corpus[seed_id]
  print('> "{}"'.format(title_corpus[seed_id]))
  similarities = [doc_similarity(seed_doc, doc) for id, doc in enumerate(corpus)]
  top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i])[-k:] # https://stackoverflow.com/questions/13070461/get-indices-of-the-top-n-values-of-a-list
  nearest = [[title_corpus[id], similarities[id]] for id in top_indices]
  print()
  for story in reversed(nearest):
    print('* "{}" ({})'.format(story[0], story[1]))

"""#Test our program"""

corpus , title_corpus = get_corpus()
vocab = get_vocab(corpus).most_common()
remove_stop(vocab,stop_list)
vocab = vocab[0:1000]
df = np.array(df_list())
#print(vocab)
k_similar(856034%1000, 5)