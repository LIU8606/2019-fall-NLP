# -*- coding: utf-8 -*-
"""lab2-0856034.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EcE3sRj9Y-HJzV_Wm4N_k8q0Q2KvYoBL
"""

import pandas as pd
import numpy as np
from collections import Counter

def get_corpus(csv_file):
  df = pd.read_csv(csv_file) # https://bit.ly/nlp-reuters
  print("Dataset columns", df.columns)
  print("Dataset size", len(df))
  corpus = df.content.to_list()
  title_corpus = df.title.to_list()
  return corpus, title_corpus

"""#Q1"""

import nltk
from nltk.tokenize import word_tokenize
from nltk import ngrams
from nltk import pos_tag
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

def get_two_gram():
  two_gram = Counter()
  for doc in corpus:
    tokens = word_tokenize(doc)
    ngram = list(ngrams(tokens, 2))
    two_gram.update(ngram)

  return two_gram.most_common()

"""Since search all 2-gram will cost a lot of time.

`get_5_most_frequent` find out 5 most frequent 2-gram then return.
"""

def get_5_most_frequent(two_gram):
  count = 0
  two_gram = np.array(two_gram)
  for token in two_gram[:,0]:
    tag = pos_tag(token)
    if tag[0][1] == "NNP" and tag[1][1] == "NNP":
      count += 1
      print(token)
    if count == 5:
      return

corpus, title_corpus = get_corpus("https://raw.githubusercontent.com/bshmueli/108-nlp/master/reuters.csv")
two_gram = get_two_gram()
get_5_most_frequent(two_gram)

"""#Q2"""

import spacy
import math
nlp = spacy.load("en_core_web_sm")

"""`del_nan` delete the document which content is "nan""""

def del_nan(corpus,title_corpus):
  c = corpus.copy()
  t = title_corpus.copy()
  for index in range(len(corpus)): 
    if type(corpus[index]) == float:  
      c.remove(corpus[index])
      t.remove(title_corpus[index])
  return c, t

"""#tokenize
token: lemma + pos_tag

`corpus_in_token` store the corpus in token type
"""

def get_token(corpus):
  tokens = []
  corpus_in_token = []
  for document in corpus:
    document_in_token = []
    doc = nlp(document)
    for word in doc:
      if not word.is_stop and not word.is_punct and not word.is_space:
        token = word.lemma_+"_"+ word.pos_
        tokens.append(token)
        document_in_token.append(token)
    corpus_in_token.append(document_in_token)

  return tokens, corpus_in_token

"""Computing word frequencies"""

def get_vocab(tokens):
  vocabulary = Counter(tokens)
  return vocabulary

"""#Compute TF_IDF Vector"""

def df_list():
  df = []
  for token,freq in vocab:
    df_count = 0
    for document in corpus_in_token:
      if token in document:
        df_count +=1
    df.append(df_count)

  return df

def TFIDFvec(doc):

  tf = [doc.count(token) for token,freq in vocab]
  tf = np.array(tf)
  tf = tf/len(doc)
  tfidf = tf * np.log(len(corpus)/df)

  return list(tfidf)

"""Cosine similarity between two numerical vectors"""

def cosine_similarity(vec_a, vec_b):
  assert len(vec_a) == len(vec_b)
  if sum(vec_a) == 0 or sum(vec_b) == 0:
    return 0 # hack
  a_b = sum(i[0] * i[1] for i in zip(vec_a, vec_b))
  a_2 = sum([i*i for i in vec_a])
  b_2 = sum([i*i for i in vec_b])
  return a_b/(math.sqrt(a_2) * math.sqrt(b_2))

def doc_similarity(doc_a, doc_b):
  return cosine_similarity(TFIDFvec(doc_a), TFIDFvec(doc_b))

"""#Find Similar Documents
Find and print the  𝑘  most similar title to a given content
"""

def k_similar(seed_id, k):
  seed_doc = corpus_in_token[seed_id]
  print('> "{}"'.format(title_corpus[seed_id]))
  similarities = [doc_similarity(seed_doc, doc) for id, doc in enumerate(corpus_in_token)]
  top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i])[-k:] # https://stackoverflow.com/questions/13070461/get-indices-of-the-top-n-values-of-a-list
  nearest = [[title_corpus[id], similarities[id]] for id in top_indices]
  print()
  for story in reversed(nearest):
    print('* "{}" ({})'.format(story[0], story[1]))

corpus, title_corpus = get_corpus("https://raw.githubusercontent.com/bshmueli/108-nlp/master/buzzfeed.csv")
corpus, title_corpus = del_nan(corpus, title_corpus) 
tokens, corpus_in_token = get_token(corpus)
vocab = get_vocab(tokens).most_common(512)
df = np.array(df_list())
k_similar(856034%1000, 5)